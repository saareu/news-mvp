import sys
import re
import pandas as pd
from pathlib import Path

# Add the src directory to the path to import schemas
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    from news_mvp.schemas import (
        validate_article_data,
    )
except ImportError as e:
    print(f"Error importing schemas: {e}", file=sys.stderr)
    print("Make sure you're running from the project root", file=sys.stderr)
    sys.exit(1)


"""
CSV Data Validator for News ETL Pipeline
========================================

Validates CSV files generated by the news ETL pipeline using centralized schema definitions.
Uses Pydantic models and schema validation for comprehensive data quality checks.

Usage:
    python scripts/ge_validate.py <csv_file_path>

Exit Codes:
    0: Validation passed
    2: Validation failed

Validates:
    - Required columns: id, title, pubDate
    - Non-null values in required columns
    - Unique IDs
    - Valid URLs in guid column (if present)
    - Parseable publication dates
    - No duplicates based on title + pubDate + source combination
    - Schema compliance using Pydantic models

Part of the news-mvp ETL quality assurance workflow.
"""


def validate(path: str) -> int:
    """Validate CSV using centralized schema definitions.

    Uses Pydantic models and schema definitions for comprehensive validation.
    """
    try:
        df = pd.read_csv(path, encoding="utf-8", engine="python")
    except Exception as e:
        print(f"Error reading CSV file: {e}", file=sys.stderr)
        return 2

    total = len(df)

    # Check for required columns based on schema
    required_cols = ["id", "title", "pubDate"]  # Core required columns for CSV
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        print(
            {
                "ok": False,
                "reason": "missing_required_columns",
                "missing": missing,
                "stats": {"total": total},
            }
        )
        return 2

    # Check for non-null values in required columns
    not_null_ok = bool(all(df[c].notna().all() for c in required_cols))

    # Check for unique IDs
    unique_ok = df["id"].nunique() == total

    # Check for duplicates based on title + pubDate + source combination
    duplicate_check_cols = ["title", "pubDate", "source"]
    missing_dup_cols = [c for c in duplicate_check_cols if c not in df.columns]
    if missing_dup_cols:
        print(
            f"Warning: Missing columns for duplicate check: {missing_dup_cols}",
            file=sys.stderr,
        )
        duplicates_ok = True  # Can't check if columns are missing
        duplicate_count = 0
    else:
        # Create a combined key for duplicate detection
        df["duplicate_key"] = (
            df["title"].astype(str)
            + "|"
            + df["pubDate"].astype(str)
            + "|"
            + df["source"].astype(str)
        )
        duplicate_count = total - df["duplicate_key"].nunique()
        duplicates_ok = duplicate_count == 0

    # Validate publication dates
    try:
        pd.to_datetime(df["pubDate"], errors="raise")
        date_ok = True
    except Exception:
        date_ok = False

    # Validate guid URLs if guid column exists
    guids_ok = True
    if "guid" in df.columns:
        url_re = re.compile(r"^https?://")
        # Only validate non-null guid values
        guid_series = df["guid"].dropna().astype(str)
        if len(guid_series) > 0:
            guids_ok = bool(guid_series.apply(lambda v: bool(url_re.match(v))).all())

    # Use schema validation for additional checks
    try:
        # Use Pydantic validation directly on CSV data
        validated_df = validate_article_data(df)
        schema_validation_ok = len(validated_df) > 0

    except Exception as e:
        print(f"Schema validation error: {e}", file=sys.stderr)
        schema_validation_ok = False

    all_ok = (
        not_null_ok
        and unique_ok
        and date_ok
        and guids_ok
        and schema_validation_ok
        and duplicates_ok
    )

    print(
        {
            "ok": all_ok,
            "stats": {
                "total": total,
                "unique_id": df["id"].nunique(),
                "date_ok": date_ok,
                "guids_ok": guids_ok,
                "schema_validation_ok": schema_validation_ok,
                "duplicates_ok": duplicates_ok,
                "duplicate_count": duplicate_count,
            },
        }
    )
    return 0 if all_ok else 2


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python scripts/ge_validate.py <csv>", file=sys.stderr)
        sys.exit(2)
    sys.exit(validate(sys.argv[1]))
